{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Case Study:\n",
    "finetune Falcon LLM for creating midjourney prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: \"Midjourney prompt for a fantasy village night\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: \"a beautiful adorable fantasy village the ground is lit like warm daylight, but the skay is dark and full of stars. Photorealistic --ar 16:9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to choose a model, in our case we will use falcon from hugging face [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:31<00:00, 15.55s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\n",
      "Daniel: Hello, Girafatron!\n",
      "Girafatron: \"Oh, hello, Daniel! How's the weather out there?\"\n",
      "Daniel: \"Oh, I'm fine, just enjoying the lovely view.\"\n",
      "Girafatron: \"Aha! What a great way to put it! Yes, I'm so happy just looking out at all the giraffes around me!\"\n"
     ]
    }
   ],
   "source": [
    "# Here, the code imports necessary libraries: AutoTokenizer and AutoModelForCausalLM from the \n",
    "# Hugging Face Transformers library, as well as the transformers, torch libraries.\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "# Using `low_cpu_mem_usage=True` or a `device_map` requires accelerate\n",
    "\n",
    "# It specifies the pre-trained language model to be used (\"tiiuae/falcon-7b-instruct\") \n",
    "# and initializes the tokenizer for that model.\n",
    "model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# It sets up a text generation pipeline using the specified model and tokenizer. \n",
    "# Several additional parameters are also set:\n",
    "#torch_dtype: The datatype used for torch tensors (set to torch.bfloat16).\n",
    "#trust_remote_code: Allows trusting remote code (set to True).\n",
    "#device_map: Automatically selects the device for computation (set to \"auto\").\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# It generates text sequences using the pipeline. The input text is provided, and several parameters are set:\n",
    "\n",
    "# max_length: Maximum length of the generated sequences (set to 200).\n",
    "# do_sample: Whether to use sampling during generation (set to True).\n",
    "# top_k: The number of highest probability words to sample from during generation (set to 10).\n",
    "# num_return_sequences: Number of sequences to generate (set to 1).\n",
    "# eos_token_id: ID of the end-of-sequence token.\n",
    "# sequences = pipeline(\n",
    "   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legendary-ml-vETwCYDa-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
